{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23571a1c-c158-4c15-803a-472e8696db63",
   "metadata": {},
   "source": [
    "<p>\n",
    "  <center><img src=\"https://drive.google.com/uc?id=10p3_7k3SlQSy0H9rUHieSdBWKI1_fArr\" width=\"500\" height=\"300\"></center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<br>\n",
    "  <center><img src=\"https://drive.google.com/uc?id=1nzy9UrGT3UCRQd58Q_oHzDK1ATAzGGLF\" width=\"600\" height=\"300\"></center>\n",
    "</p>\n",
    "<br>\n",
    "<h1 style='text-align: center;'> سلسلة معسكرات ذكاء: المعسكر الثاني | استكشاف وتحليل البيانات وأساسيات تعلم الآلة </h1>\n",
    "<!-- <font size=\"+2\" style='text-align: center;'> تعلّم كيف تكتشف البيانات وتطوّر نماذج تعلّم الآلة </h2> -->\n",
    "<div style=\"text-align: center; font-family: Arial; font-size: 2em;\">تعلّم كيف تكتشف البيانات وتطوّر نماذج تعلّم الآلة</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07305093-8b5a-4996-8584-87eac3f62611",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2 style=\"text-align: left;\"> 📚 استيراد المكتبات</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abdf586-bf6b-4a60-8ea4-dc4374fd15ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae4a01-fd38-4dc8-a385-a0fff3e6d3cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2 style=\"text-align: left;\">🚗 تحميل وتجهيز البيانات</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391748cd-bf76-42cc-adb6-abb3423f1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://drive.google.com/file/d/1qTcYk4dJzMfpxlba6lZ0XfbGHvmlqyLb/view?usp=sharing'\n",
    "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "df = pd.read_csv(url, usecols=['Mileage', 'Year', 'Price'])\n",
    "\n",
    "df = df[df['Price'] != 'Negotiable']\n",
    "df['Price'] = df['Price'].apply(int)\n",
    "df[\"class\"] = df[\"Price\"] > 50000\n",
    "df.drop(columns=\"Price\", inplace=True)\n",
    "df[\"class\"] = df[\"class\"].replace([False,True],[0,1])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d575af2f-7ab6-4301-ad2c-7349c1aa0049",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: left;\">🔪 تقسيم البيانات إلى جزء للتدريب وآخر للتقييم</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9bdf4-e89e-4ff0-af9e-4a9f64234864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_test_split_frac = 0.8\n",
    "random_seed_fix = 47\n",
    "np.random.seed(random_seed_fix)\n",
    "msk = np.random.rand(len(df)) < train_test_split_frac\n",
    "\n",
    "df[\"flag_train\"] = 0\n",
    "df.loc[msk, \"flag_train\"] = 1\n",
    "\n",
    "train_df = df[(df[\"flag_train\"] == 1)].drop([\"flag_train\"], axis=1).copy()\n",
    "test_df = df[(df[\"flag_train\"] == 0)].drop([\"flag_train\"], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c9104-19af-48d4-b7b7-9a0de3c6752a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c530868-16e9-4c8f-937e-cc84f70b7596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32ae7e-89a2-428d-8d2b-3118b1c4894f",
   "metadata": {},
   "source": [
    "## Scikit-learn Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19094d65-f9bf-49a9-bda4-691918080b98",
   "metadata": {},
   "source": [
    "### Component#1: Data Imputation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf1143-cd08-4087-8c0e-46bc012f91b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: initialize a data imputer that uses the median\n",
    "### Your code goes here\n",
    "imputer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4239a-3ea1-41a1-bf9b-ceaf25d03086",
   "metadata": {},
   "source": [
    "### Component#2: Data Scalling\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8eb7b-0a78-4abb-917d-80a4f6030944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: initialize a data scaler\n",
    "### Your code goes here\n",
    "scaler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df03f8-56fc-45b9-9479-44f95797b73b",
   "metadata": {},
   "source": [
    "### Component#3: Calssifier\n",
    "\n",
    "https://scikit-learn.org/stable/supervised_learning.html#supervised-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de40f13-34e4-46a6-8fa6-e684ff9e0eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: initialize a classifier\n",
    "### Your code goes here\n",
    "classifier = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4977599-0e66-4128-bd0b-eb2c58498eea",
   "metadata": {},
   "source": [
    "### Constructing and training the pipeline\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f539e-f68c-4119-b150-cb4220872e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_pipeline(df):\n",
    "    # TODO: Crate a list of tuples that includes the components you initialized above\n",
    "    # Hint: Check out Scikit-learn's pipeline documentation\n",
    "    ### Your code goes here\n",
    "    estimators = None\n",
    "    \n",
    "    # TODO: Create the Scikit-learn's pipeline\n",
    "    ### Your code goes here\n",
    "    pipeline = None\n",
    "    feature_columns = df.columns.values[df.columns.values != \"class\"]\n",
    "    pipeline.fit(df[feature_columns], df[\"class\"])\n",
    "\n",
    "    return pipeline, feature_columns\n",
    "\n",
    "pipeline, feature_columns = train_pipeline(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb83b77-5779-4d13-9405-e359329d8722",
   "metadata": {},
   "source": [
    "### Saving the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215162a-a717-4084-98f3-b351ac4fdeae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_pipeline(folder_path, pipeline, feature_columns):\n",
    "    # Dump .... \n",
    "    # save the pipeline\n",
    "    pipeline_name = \"pipeline.pkl\" \n",
    "    pipeline_path = Path(folder_path, pipeline_name)\n",
    "    \n",
    "    ## TODO: Write an one-line code that saves the pipeline as a pickle file\n",
    "    # Hint: search in the joblib package about the function joblib.dump()\n",
    "    ### Your code goes here\n",
    "    pass\n",
    "\n",
    "    # save the feature_columns\n",
    "    cols_name = \"pipeline_cols.pkl\"\n",
    "    cols_path = Path(folder_path, cols_name)\n",
    "    ## TODO: Write an one-line code that saves the feature columns as a pickle file\n",
    "    # Hint: search in the joblib package about the function joblib.dump()\n",
    "    ### Your code goes here\n",
    "    pass\n",
    "    \n",
    "output_path = Path(\"pipeline-output\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_pipeline(output_path, pipeline, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb178598-19b9-4201-9c2d-e1ff0e7ad259",
   "metadata": {},
   "source": [
    "### Loading the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e333b8-c254-4065-9bc7-0f50f110bca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pipeline(folder_path):\n",
    "    # load the pipeline\n",
    "    model_name = \"pipeline.pkl\"\n",
    "    model_path = Path(folder_path, model_name)\n",
    "    ## TODO: Write an one-line code that load the pipeline from a pickle file\n",
    "    # Hint: search in the joblib package about the function joblib.load()\n",
    "    ### Your code goes here\n",
    "    pass\n",
    "\n",
    "    cols_name = \"pipeline_cols.pkl\"\n",
    "    cols_path = Path(folder_path, cols_name)\n",
    "    ## TODO: Write an one-line code that load the feature columns from a pickle file\n",
    "    # Hint: search in the joblib package about the function joblib.load()\n",
    "    ### Your code goes here\n",
    "    pass\n",
    "\n",
    "    return pipeline, feature_columns\n",
    "\n",
    "# Load the model, the scaler, the imputer and the names of the columns from files:\n",
    "pipeline, feature_columns = load_pipeline(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744fb16-df51-47b5-8ed3-2ef2006bafae",
   "metadata": {},
   "source": [
    "### Do predictions using the trained pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434d391-abb5-4b19-b295-f5c9951ac911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_pipeline(test_df, pipeline, feature_columns):\n",
    "    # TODO: Compute predictions using the trained pipeline\n",
    "    # Write a line that predicts the y values for the test data using the pipeline\n",
    "    # Hint: Look for method that predicts and use the feature_columns saved to restrict the used features.\n",
    "    y_hat = None\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "# Use the loaded model and other logic to get predictions for test-data in `test_df`:\n",
    "pipeline_y_hat = predict_pipeline(test_df, pipeline, feature_columns)\n",
    "pipeline_y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb8ef2-51da-4e12-86da-eac0acf767ca",
   "metadata": {},
   "source": [
    "## ONNX (Extra)\n",
    "\n",
    "That was fun, but probably not that useful to any of you. However we added it to highlight two things. Firstly, the fact that you can move away from Python to deliver the model, while still developing it in Python. And secondly, or probably most imporantly, that the code and the weights are inextricably linked!\n",
    "\n",
    "So while that was one lesser used method, let's go through the most predominant method for making model artifacts from Python to any platform you want. This is ONNX - Open Neural Network eXchange, and as the name suggests it was developed as an open standard for sharing neural networks. However, it now supports traditional ML models, like the ones we have been making in this course.\n",
    "\n",
    "We will need to use the `skl2onnx` library. There are two ways to convert the model. A first method `convert_sklearn` which is a comprehensive way of converting. A second `to_onnx` which is calls `convert_sklearn`, but with simplified parameters and allows the option to infer the information from the training data. We won't show `to_onnx`, you can do that yourself.\n",
    "\n",
    "To start with we will use `convert_sklearn`, and as such we need to think about the datatype that we will be inputting. Lets have a look at what goes into the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0596613-29d1-4cc6-8d62-d54a42f02634",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install skl2onnx onnxruntime pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0726be86-1771-457a-8272-b06b4a9b668e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "onnx_df = train_df.copy()\n",
    "\n",
    "onnx_df[onnx_df.columns] = onnx_df[onnx_df.columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "onnx_feature_columns = onnx_df.columns.values[onnx_df.columns.values != \"class\"]\n",
    "\n",
    "onnx_df.iloc[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49039f4c-b0e7-4edd-b457-27cb6ad0e207",
   "metadata": {},
   "source": [
    "We can check the type using numpy's `dtype`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345fd46-7591-4bee-9e93-9056dd1f3ecc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "onnx_df.iloc[0].values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57e5af-bd6b-4e06-9079-f3d8f3b5f585",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(onnx_df.iloc[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990d9b7-061b-42a6-bfb5-b3c232bb9b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558ad25-915f-4453-9dd2-b4520a361e42",
   "metadata": {},
   "source": [
    "And will get it to be a `float64` type. Okay! We know enough to start up. Lets import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4af10-26e8-44ad-ab8a-bbc33097e92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27397307-e51a-459b-904f-75a37b535f79",
   "metadata": {},
   "source": [
    "The `initial_types` variable might take some getting used to. Assume that the specified scikit-learn model takes an input where the first 10 elements are floats and the last 100 elements are integers. For `convert_sklearn` we need to specify initial types. These mean importing `FloatTensorType` and `Int64TensorType` for the two different types:\n",
    "\n",
    "```Python\n",
    "from skl2onnx.common.data_types import FloatTensorType, Int64TensorType\n",
    "```\n",
    "\n",
    "We can then define the `initial_type` as a python list, where each element is a tuple of a variable name and a type \n",
    "\n",
    "```Python\n",
    "initial_type = [('float_input', FloatTensorType([None, 10])),\n",
    "                ('int64_input', Int64TensorType([None, 100]))]\n",
    "```\n",
    "\n",
    "The `[None]` in `[None, 10]` indicates the batch size here is unknown. For our model it is super easy. The type is `Float` and they are all of that type, so its size is 170 (171 columns including the class).\n",
    "\n",
    "Lets convert the model now then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39676d50-001f-4f24-982e-373ac21d890f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "initial_type = [('float_input', FloatTensorType([None, 2]))]\n",
    "\n",
    "# Convert the model pipeline to ONNX. \n",
    "# The `pipeline` object is the same as before - so make sure you've run that code earlier in the notebook.\n",
    "onnx = convert_sklearn(pipeline, initial_types=initial_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44c817-8c70-40d8-805d-5f78de1b54c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onnx.graph.node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f97918-abc9-448e-a8c2-b94804ac8987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print only the first few characters, since the output is really long..\n",
    "print(str(onnx)[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3bd986-95d4-42f4-8c71-8b5e5073bdae",
   "metadata": {},
   "source": [
    "Have a look above and see the model format. Its kind of readible. You can also plot it as a graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc6c4eb-4f2d-4dee-a943-1fedc3f8d08d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from onnx.tools.net_drawer import GetPydotGraph, GetOpNodeProducer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed17d04-340e-4212-9266-1b4c6fa9f67b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pydot_graph = GetPydotGraph(onnx.graph,\n",
    "                            name=onnx.graph.name,\n",
    "                            rankdir=\"TB\",\n",
    "                            node_producer=GetOpNodeProducer(\n",
    "                                \"docstring\", color=\"yellow\",\n",
    "                                fillcolor=\"yellow\", style=\"filled\"))\n",
    "\n",
    "pydot_graph.write_dot(\"pipeline_onnx.dot\")\n",
    "\n",
    "os.system('dot -O -Gdpi=300 -Tpng pipeline_onnx.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180723d-fe5c-4734-b8b4-fde362ca5886",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image = plt.imread(\"pipeline_onnx.dot.png\")\n",
    "fig, ax = plt.subplots(figsize=(40, 20))\n",
    "ax.imshow(image)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050aafd0-6e35-4641-9352-ead0726a3746",
   "metadata": {},
   "source": [
    "So that was a soft introduction into saving the pipeline/model in the ONNX-format. Now we just use the `.SerializeToString()` method to write the ONNX object to file. Lets write that up in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ad434-30ea-4816-986c-f96d9bbbc2a8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def save_onnx(folder_path, pipeline, feature_columns):\n",
    "    # convert to onnx\n",
    "    initial_type = [('float_input', FloatTensorType([None, 2]))]\n",
    "    onnx = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "\n",
    "    # save the pipeline\n",
    "    pipeline_name = \"pipeline.onnx\"\n",
    "    folder_path = Path(folder_path)\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    pipeline_path = Path(folder_path, pipeline_name)\n",
    "\n",
    "    with open(pipeline_path, \"wb\") as f:\n",
    "        f.write(onnx.SerializeToString())\n",
    "\n",
    "    # save the feature_columns\n",
    "    cols_name = \"pipeline_cols.joblib\"\n",
    "    cols_path = Path(folder_path, cols_name)\n",
    "    joblib.dump(feature_columns, cols_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2961ab0-0bb7-411a-af83-3b90c6a40ca6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "save_onnx(\"onnx-output\", pipeline, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76810983-d5e2-4d15-b5c0-c114315e83db",
   "metadata": {},
   "source": [
    "Check out the results in the \"onnx-output\" folder!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1a187-ffeb-4117-be8a-3ad2da4b32fd",
   "metadata": {},
   "source": [
    "Next up we need to examine how to consume ONNX and make predictions. This is where ONNX comes into its own. There are a tonne of ways of doing this for a multitude of platforms and programs. We will look at a server-based inference with a sample REST server, using `onnxruntime`.\n",
    "\n",
    "Lets define a load function that will load the runtime session and also the feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fda8d8-78ca-4a93-9df9-1ca48c364ae3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeabb17-802d-450f-ba71-b0be7684dbdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_onnx(folder_path):\n",
    "    # load the session\n",
    "    pipeline_name = \"pipeline.onnx\"\n",
    "    pipeline_path = Path(folder_path, pipeline_name)\n",
    "    sess = rt.InferenceSession(str(pipeline_path))\n",
    "\n",
    "    # load the feature_columns\n",
    "    cols_name = \"pipeline_cols.joblib\"\n",
    "    cols_path = Path(folder_path, cols_name)\n",
    "    feature_columns = joblib.load(cols_path)\n",
    "\n",
    "    return sess, feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f7061-d8fe-4800-8a6f-6ac9fb8cfb03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the ONNX model and the feature-columns using the function we just wrote:\n",
    "sess, feature_columns = load_onnx(\"onnx-output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6b0f5-2eef-434c-b0d9-d9bf829a0f04",
   "metadata": {},
   "source": [
    "We can look at the input and output names, types expected and shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d01660-096e-4052-9386-aa9ee5998bf2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"In\", [(i.name, i.type, i.shape) for i in sess.get_inputs()])\n",
    "print(\"Out\", [(i.name, i.type, i.shape) for i in sess.get_outputs()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fad5b5-9e77-46bd-b836-997aaa5c796c",
   "metadata": {},
   "source": [
    "Here we see that we have a input called `float_input` (remember we defined this above), and its a tensor type `float`. Its expected size is any number of samples in the batch, but exactly two columns.\n",
    "\n",
    "The output is called `output_label` (we could also get a probability too) which is the same as what we saw on the graph of the model. Its an tensor of `int64` output, but in a list.\n",
    "\n",
    "Lets write a function that does the predict, when given an dataframe as input, along with the session and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97c655-af1d-43ce-a7eb-4fbf2c83a935",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_pipeline(test_df, sess, feature_columns):\n",
    "\n",
    "    sample_to_score = test_df[feature_columns].values.astype(\"float32\")\n",
    "    y_hat = sess.run(['output_label'], {'float_input': sample_to_score})\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c522af-98e2-4902-8432-726d761bfa2f",
   "metadata": {},
   "source": [
    "We can make a prediciton of the test dataframe, and see what the output looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff0401-e7ef-408f-8836-83e81c3f06d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "onnx_y_hat = predict_pipeline(test_df, sess, feature_columns)\n",
    "onnx_y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1714f-d30e-49ed-aef1-689fdafba5c6",
   "metadata": {},
   "source": [
    "## APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cdb6d-65c2-427f-9965-7483b731e5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from typing import Optional\n",
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import toml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pydantic import BaseModel,confloat\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def load_pipeline(folder_path):\n",
    "    model_name = \"pipeline.pkl\"\n",
    "    model_path = Path(folder_path, model_name)\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    cols_name = \"pipeline_cols.pkl\"\n",
    "    cols_path = Path(folder_path, cols_name)\n",
    "    feature_columns = joblib.load(cols_path)\n",
    "\n",
    "    return model, feature_columns\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Car(BaseModel):\n",
    "    year: Optional[float]=2015\n",
    "    mileage: Optional[float]=77000\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"record\": {'year':2015,\n",
    "                       'mileage':77000,\n",
    "                      }\n",
    "        }\n",
    "        \n",
    "@app.on_event(\"startup\")\n",
    "def load_model():\n",
    "    global pipeline\n",
    "    global feature_columns\n",
    "\n",
    "    output_path = Path(\"pipeline-output\")\n",
    "    pipeline, feature_columns = load_pipeline(output_path)\n",
    "    \n",
    "@app.get('/')\n",
    "def index():\n",
    "    return {\"status\": \"pass\"}\n",
    "\n",
    "\n",
    "@app.post('/predict')\n",
    "def get_price_classificatoon(data: Car):\n",
    "    recieved = data\n",
    "    print(data)\n",
    "    year = recieved.year\n",
    "    mileage = recieved.mileage\n",
    "    X = np.array([year, mileage]).reshape(1, -1)\n",
    "    pred_class = pipeline.predict(X)\n",
    "    pred_proba = pipeline.predict_proba(X)\n",
    "    if(pred_class==0):\n",
    "        return {\"Class\":\"Car Price <50k SAR\",\"Probability\":round(float(pred_proba[0][0]),3)}\n",
    "    else:\n",
    "        return {\"Class\":\"Car Price >=50k SAR\",\"Probability\":round(float(pred_proba[0][1]),3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bc016-8c44-4d53-95cc-91826031f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi nest-asyncio pyngrok uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37454dc8-ed02-43a6-bee2-07ef63cc4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Public URL:', ngrok_tunnel.public_url)\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84a451-86cc-4dd8-bb3c-9c92e75da185",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X 'POST' \\\n",
    "'http://4cb9-35-226-250-83.ngrok.io/predict' \\\n",
    "-H 'accept: application/json' \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d '{\"year\": 2009, \"mileage\": 300000}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a8921-4f7d-448a-bb0a-3695d8fc7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X 'POST' \\\n",
    "'http://4cb9-35-226-250-83.ngrok.io/predict' \\\n",
    "-H 'accept: application/json' \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d '{\"year\": 1980, \"mileage\": 200000}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b594a4-b08a-4347-96e4-e3da41d86fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X 'POST' \\\n",
    "'http://4cb9-35-226-250-83.ngrok.io/predict' \\\n",
    "-H 'accept: application/json' \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d '{\"year\": 1980, \"mileage\": dkmd}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01307250-b692-49bf-a3a5-4503c0cd509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X 'POST' \\\n",
    "'http://4cb9-35-226-250-83.ngrok.io/predict' \\\n",
    "-H 'accept: application/json' \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d '{\"year\": 2008}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f389ba-2a5e-4da3-9815-976798e3fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X 'POST' \\\n",
    "'http://4cb9-35-226-250-83.ngrok.io/predict' \\\n",
    "-H 'accept: application/json' \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d '{\"year\": 2008, \"mileage\": 770000}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f06d9-9726-4cd4-9921-bc5aff0a930b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
